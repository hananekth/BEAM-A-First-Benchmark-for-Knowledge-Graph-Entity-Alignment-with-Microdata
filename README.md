# BEAM: A First Benchmark for Knowledge Graph Entity Alignment with Microdata

This repository contains the **code, data, and preprocessing pipeline** for the benchmark introduced in the paper:
**BEAM : Un premier benchmark pour lâ€™alignement des microdonnÃ©es du web avec les graphes de connaissances** (ğŸ“„ link to be added).

The benchmark aligns **Web Data Commons (WDC) macrodata** with **Wikidata** using **key-based matching** (e.g., IATA codes for airports, ISBN for books). Unlike synthetic benchmarks (e.g., DBP15K, OpenEA datasets), MEAB preserves the **noise, heterogeneity, and incompleteness** of real-world data, offering a more realistic evaluation for entity alignment (EA).

Our contributions:

* ğŸ—ï¸ Provide **class-specific datasets** (currently: *airports* and *books*).
* âš™ï¸ Include a **preprocessing pipeline** to reproduce or extend the benchmark to new classes.
* ğŸ“Š Evaluate several **EA models** (MTransE, AliNet, AlignE, AttrE, GCNAlign, BootEA) under the same hyperparameters as OpenEA.
* ğŸ”— Supply **ground truth alignments** via *key-based matching* (IATA/ISBN), instead of rare or noisy `owl:sameAs` links.
* ğŸŒ Release the dataset under FAIR principles â€” Findable, Accessible, Interoperable, Reusable.

A visualization and navigation tool for exploring the datasets:
ğŸ”— [rust-kg-explorer](https://github.com/bareyan/rust-kg-explorer)

---

## ğŸ“¦ Repository Structure

```bash
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ airport/
â”‚   â”‚   â”œâ”€â”€ rel_triples_1
â”‚   â”‚   â”œâ”€â”€ rel_triples_2
â”‚   â”‚   â”œâ”€â”€ attr_triples_1
â”‚   â”‚   â”œâ”€â”€ attr_triples_2
â”‚   â”‚   â””â”€â”€ ent_links
â”‚   â””â”€â”€ books/
â”‚       â”œâ”€â”€ rel_triples_1
â”‚       â”œâ”€â”€ rel_triples_2
â”‚       â”œâ”€â”€ attr_triples_1
â”‚       â”œâ”€â”€ attr_triples_2
â”‚       â””â”€â”€ ent_links
â”‚   
â”œâ”€â”€ args/
â”‚   â””â”€â”€ *.json   # example configs for running models
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ wdc/         # WDC-specific cleaning
â”‚   â”œâ”€â”€ wikidata/    # Wikidata extraction & filtering
â”‚   â””â”€â”€ entity_linking/
â”œâ”€â”€ scripts/
â””â”€â”€ README.md
```

---

## âš™ï¸ Pipeline Overview

Each major preprocessing step has **two implementations**:

1. âœ… **Original (bash, Python, awk, grep, etc.)** â€” lightweight and reproducible on any system.
2. ğŸ˜ **Optional PostgreSQL scripts** â€” for users who prefer relational DB processing at scale, or who like to use the rust-KG explorer tool.

The pipeline includes:

1. **Extract WDC triples** (airports, books).
2. **Clean and filter**: remove irrelevant predicates (e.g., image, logo, hasMap).
3. **Class focus**: enforce typing (`schema.org/Airport`, `schema.org/Book`).
4. **Entity filtering**: drop low-frequency or minimal-information entities.
5. **Key-based merging**: merge duplicates by keys (e.g., IATA, ISBN).
6. **Wikidata extraction**: fetch entities and properties via SPARQL.
7. **Property filtering**: retain high-frequency, relevant attributes.
8. **Entity linking**: generate ground-truth alignment based on keys (IATA/ISBN).

---
Perfect â€” thanks for clarifying! Since you want to keep the README as youâ€™ve drafted it, but **add the detailed preprocessing steps (Original + SQL)**, we can slot them right after the **Pipeline Overview** section, preserving your style.

Hereâ€™s how the continuation should look (you can paste it directly under your â€œPipeline Overviewâ€ section):

---

## ğŸ”„ Preprocessing Steps

Below we illustrate the main preprocessing stages for **Airports** (Books follow the same logic, with ISBN instead of IATA).
Each step has both the âœ… **original approach** (bash/Python/awk/grep) and the ğŸ˜ **PostgreSQL version**.

---

### ğŸ“ 1. Extract Initial Triples

âœ… Original:

```bash
python preprocessing/WDC/create_wdc_triples.py 
python preprocessing/WDC/get_wdc_airports.py
```

ğŸ˜ PostgreSQL:

```sql
CREATE TABLE wdc_triples_raw (
    subject TEXT,
    predicate TEXT,
    object TEXT
);
COPY wdc_triples_raw FROM '/path/to/wdc_airport_related_triples.txt' DELIMITER E'\t';
```

---

### ğŸ“ 2. Clean and Filter (remove logos, images, sameAs, etc.)

âœ… Original:

```bash
grep '^_:' wdc_airport_related_triples.txt | \
grep -v '<http://schema.org/image>' | \
grep -v '<http://schema.org/sameAs>' | \
grep -v '<http://schema.org/logo>' > path/to/cleaned/depth0_wdc.txt
```

ğŸ˜ PostgreSQL:

```sql
CREATE TABLE wdc_cleaned_1 AS
SELECT * FROM wdc_triples_raw
WHERE subject LIKE '_:%'
  AND predicate NOT IN (
    '<http://schema.org/image>',
    '<http://schema.org/sameAs>',
    '<http://schema.org/logo>'
  );
```

---

### ğŸ“ 3. Keep Only Type Airport

âœ… Original:

```bash
awk -F'\t' '$2 ~ /type/ && $3 !~ /schema.org\/Airport/ {print $1}' cleaned_depth0_wdc.txt | sort | uniq > bad_ids.txt
grep -vFf bad_ids.txt cleaned_depth0_wdc.txt > cleaned2_depth0.txt
```

ğŸ˜ PostgreSQL:

```sql
CREATE TEMP TABLE non_airports AS
SELECT DISTINCT subject
FROM wdc_cleaned_1
WHERE predicate LIKE '%type%'
  AND object NOT LIKE '%schema.org/Airport%';

CREATE TABLE wdc_cleaned_2 AS
SELECT *
FROM wdc_cleaned_1
WHERE subject NOT IN (SELECT subject FROM non_airports);
```

---

### ğŸ“ 4. Remove Low-Frequency Entities

âœ… Original:

```bash
cut -f1 cleaned2_depth0.txt | sort | uniq -c | sort -n > freq.txt
awk '$1 == 2 {print $2}' freq.txt > min_ids.txt
grep -vFf min_ids.txt cleaned2_depth0.txt > cleaned3_depth0.txt
```

ğŸ˜ PostgreSQL:

```sql
CREATE TEMP TABLE subject_counts AS
SELECT subject, COUNT(*) AS freq
FROM wdc_cleaned_2
GROUP BY subject;

CREATE TEMP TABLE min_ids AS
SELECT subject FROM subject_counts WHERE freq = 2;

CREATE TABLE wdc_cleaned_3 AS
SELECT * FROM wdc_cleaned_2
WHERE subject NOT IN (SELECT subject FROM min_ids);
```

---

### ğŸ“ 5â€“10. Further Cleaning

* **Step 5:** remove specific outlier subjects.
* **Step 6:** remove `hasMap` and `mainEntityOfPage`.
* **Step 7:** remove subjects with frequency = 2.
* **Step 8:** remove `schema.org/url`, keep only English names.
* **Step 9:** drop subjects with only 2â€“3 triples.
* **Step 10:** add more depth if there is linked entities in the same folder.

âœ… Each step uses `grep`, `awk`, and filtering commands.
ğŸ˜ Equivalent SQL tables are built step-by-step (see paper and scripts for full details).

---

### ğŸ“ 11. Wikidata Extraction and Filtering


```bash
python preprocessing/Wikidata/d1_wiki.py  #get triples
python preprocessing/Wikidata/filter_wiki_basedOn_props.py #filter them
python preprocessing/Wikidata/d1_wiki.py #add depth 1 info about the properties in wikidata
```
---

### ğŸ“ 12. Entity Linking via Keys


```bash
python preprocessing/entity_linking/get_new_ent_iata_links.py
```

---


### ğŸ“ 13. Create training, testing and validation folders


```bash
./scripts/create_folds.sh  
```

---

ğŸ“Œ For the **Books** dataset, replace IATA with **ISBN** keys, but the workflow is identical.

---

## ğŸ“Š Dataset Statistics

### Example (Airport benchmark)

| Dataset                 | Attribute Triples | Relational Triples | Links |
| ----------------------- | ----------------- | ------------------ | ----- |
| WDC (airport)           | 6,728             | 28,973             |       |
| Wikidata (airport)      | 61,090            | 163,517            |       |
| Ground truth alignments | â€“                 | â€“                  | 2,526 |

Similar statistics are available for the *books* class.

---

## ğŸ“ˆ Model Evaluation

We tested six representative EA models (all from [OpenEA](https://github.com/nju-websoft/OpenEA)):

* **MTransE** (translation-based)
* **AliNet** (multi-hop neighborhood attention)
* **AlignE** (attribute embedding)
* **AttrE** (attribute character embedding)
* **GCNAlign** (graph convolutional alignment)
* **BootEA** (bootstrapping with iterative refinement)

On curated datasets like DBP15K, models reach Hits\@5 of **40â€“60%**.
On MEAB, performance drops to **\~1â€“2% Hits\@5**, confirming the difficulty of aligning noisy, semi-structured web macrodata.

---

## ğŸ“Œ Notes

* Current benchmark covers **Airports** and **Books**; the pipeline generalizes to new classes.
* Ground truth is constructed via **key-based matching** (IATA / ISBN), not `owl:sameAs`.
* Preprocessing deliberately preserves **noise, duplicates, and heterogeneity** to reflect reality.

---

## ğŸ“„ License

This project is released under the **CC-BY License**, consistent with the data sources. (to be modified)

---

## âœ¨ Acknowledgments

We thank the creators of **Web Data Commons** and **Wikidata** for making the data publicly available.
This work is part of and supported by the *mekano* project.

---

## ğŸ”— Related Projects

* [OpenEA](https://github.com/nju-websoft/OpenEA) â€“ EA model implementations.
* [rust-kg-explorer](https://github.com/bareyan/rust-kg-explorer) â€“ GUI tool for visualizing the datasets.

---

